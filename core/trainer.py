from ..network.DualNetwork import *
from ..utils.third import *
from ..network.losses import *
from ..utils.image_pool import ImagePool
from torch.utils.tensorboard import SummaryWriter
import os
from scipy.io import savemat
from tqdm import tqdm
class PND_NET_trainer(nn.Module):
    def __int__(self):
        super(PND_NET_trainer, self).__int__()

    def initial(self, opt, device, dataset, train_loder, val_loder=None):
        self.opt = opt
        self.device = device
        #dataset
        self.dataset = dataset
        self.train_loder = train_loder
        if val_loder:
            self.val_loder = val_loder
        else:
            self.val_loder = None

        self.CombineNet = CombineNet(self.opt).to(device=self.device)
        self.DecompoNet = DecomposeNet(self.opt).to(device=self.device)

        self.optimizer_G = torch.optim.Adam(itertools.chain(self.CombineNet.parameters(), self.DecompoNet.parameters()), lr = self.opt.DuaModel_lr,
                                        betas=(0.5, 0.999), eps=1e-06)
        self.scheduler_G = torch.optim.lr_scheduler.LambdaLR(self.optimizer_G,
                                                             lr_lambda=LambdaLR(opt.epochs+100, 0, opt.epochs//2).step)

        # Discriminator
        self.netD_Xc = Discriminator(input_nc=1).to(device=self.device)
        self.netD_Xa = Discriminator(input_nc=1).to(device=self.device)
        self.optimizer_D = torch.optim.Adam(itertools.chain(self.netD_Xa.parameters(),self.netD_Xc.parameters()),lr = self.opt.D_lr,
                                                betas=(0.5, 0.999), eps=1e-06)
        self.scheduler_D = torch.optim.lr_scheduler.LambdaLR(self.optimizer_D,
                                                             lr_lambda=LambdaLR(opt.epochs + 100, 1, opt.epochs//2).step)

        # loss
        self.criterionIdt = torch.nn.L1Loss(reduction='mean')
        self.criterionCycle = torch.nn.L1Loss(reduction='mean')
        self.tv = TotalVariationLoss(c_img=1).to(device)
        self.criterionGAN = GANLoss(gan_mode='lsgan').to(device=self.device)

        self.fake_Xa_pool = ImagePool(self.opt.pool_size)
        self.fake_Xc_pool = ImagePool(self.opt.pool_size)

        # train loss logger
        self.dis_writer = SummaryWriter(
            os.path.join(self.opt.loggerpath, 'dis'))
        self.gen_writer = SummaryWriter(
            os.path.join(self.opt.loggerpath, 'gen'))

        # gaussian kernel
        self.GaussianFilter3 = GaussianFilterLayer(1, 5, 3, 1).to(self.device)

        self.FBPtool = FBP
        self.FPtool = FP


    def forward(self):
        pass

    def set_requires_grad(self, nets, requires_grad=False):
        """Set requies_grad=Fasle for all the networks to avoid unnecessary computations
        Parameters:
            nets (network list)   -- a list of networks
            requires_grad (bool)  -- whether the networks require gradients or not
        """
        if not isinstance(nets, list):
            nets = [nets]
        for net in nets:
            if net is not None:
                for param in net.parameters():
                    param.requires_grad = requires_grad

    def backward_D_basic(self, netD, real, fake):
        """Calculate GAN loss for the discriminator

        Parameters:
            netD (network)      -- the discriminator D
            real (tensor array) -- real images
            fake (tensor array) -- images generated by a generator

        Return the discriminator loss.
        We also call loss_D.backward() to calculate the gradients.
        """
        # Real
        pred_real = netD(real)
        loss_D_real = self.criterionGAN(pred_real, True)
        # Fake
        pred_fake = netD(fake.detach())
        loss_D_fake = self.criterionGAN(pred_fake, False)
        # Combined loss and calculate gradients
        loss_D = (loss_D_real + loss_D_fake) * 0.5
        loss_D.backward()
        return loss_D

    def D_Xa_backward(self):
        fake_Xa = self.fake_Xa_pool.query(self.Xca)
        self.loss_D_Xa = self.backward_D_basic(self.netD_Xa,self.Xa,fake_Xa)

    def D_Xc_backward(self):
        fake_Xc = self.fake_Xc_pool.query(self.X_ac)
        self.loss_D_Xc = self.backward_D_basic(self.netD_Xc,self.Xc,fake_Xc)

    def _train_mode(self):
        self.CombineNet.train()
        self.DecompoNet.train()
        self.netD_Xa.train()
        self.netD_Xc.train()

    def _eval_mode(self):
        self.CombineNet.eval()
        self.DecompoNet.eval()
        self.netD_Xa.eval()
        self.netD_Xc.eval()

    def updata_lr_scheduler(self):
        self.scheduler_G.step()
        self.scheduler_D.step()

    def Generator_zero_grad(self):
        self.CombineNet.zero_grad()
        self.DecompoNet.zero_grad()

    def _saveModel(self, epoch):
        save_ckpt('{:s}/{:s}.pth'.format(self.opt.save_dir, self.opt.netD_Xc_name),
                  [('model', self.netD_Xc)], [('optimizer', self.optimizer_D)], epoch)
        save_ckpt('{:s}/{:s}_epoch{:d}.pth'.format(self.opt.save_dir, self.opt.DualModle_name, epoch),
                  [('model', self.DecompoNet), ('model2', self.CombineNet)], None, epoch)

    def Generator_backward(self):
        lambda_idt = self.opt.lambda_idt*self.opt.scaleLoss
        lambda_art = self.opt.lambda_art*self.opt.scaleLoss
        lambda_cycle = self.opt.lambda_cycle*self.opt.scaleLoss
        lambda_LI_prior = self.opt.lambda_LI_prior*self.opt.scaleLoss

        #identity loss
        if lambda_idt>0:
            # Xa->FP->Sa Sa->SENet->Sc+Sm Sc->FBP->X_ic X_ic->INet->Xim
            Xcc = self.DecompoNet.forward2(self.Xc)
            self.loss_idt = self.criterionIdt(Xcc, self.Xc)
            # Xaa = self.DualModle2.forward2(self.Xa, self.NoMetalMask*0.001)
            # self.loss_idt = self.criterionMSE(Xcc,self.Xc)+self.criterionMSE(self.Xa,Xaa)
        else:
            self.loss_idt = 0

        # GAN loss
        self.loss_G_Xc = self.criterionGAN(self.netD_Xc(self.X_ac), True)
        self.loss_G_Xa = self.criterionGAN(self.netD_Xa(self.Xca), True)

        # sinogram loss
        blur_SLI = self.GaussianFilter3(self.SLI)
        blur_Sac = self.GaussianFilter3(self.Sac)
        sinogram_tv = self.tv(self.Sac)
        self.loss_idt_LIMAR = self.criterionCycle(blur_SLI, blur_Sac)

        #cycle loss
        self.L_cycle = self.criterionCycle(self.Xc,self.Xcac)+self.criterionCycle(self.Xa,self.Xaca) #+self.criterionIdt(self.Scac,self.Sc)

        self.L_art =  self.criterionCycle(self.Sm,self.Sm_cac) + self.criterionCycle(self.X_im,self.X_im_cac)

        #combined all loss
        self.loss_G = self.loss_G_Xa+self.loss_G_Xc\
                        +self.loss_idt*lambda_idt\
                        +self.loss_idt_LIMAR*lambda_LI_prior\
                        +self.L_cycle*lambda_cycle\
                        +self.L_art*lambda_art + sinogram_tv*0.1
        self.loss_G.backward()

    def Model_train(self):
        dataType = torch.float32
        for epoch in range(self.opt.epochs):
            loop =  tqdm(enumerate(self.train_loder), total=len(self.dataset)/self.opt.batch_size)
            temp_GAN_loss = []
            temp_Dis_loss = []
            for step,(Xc,_,XLI,Mask,Sc,Sa,SLI) in loop:
                self.NoMetalMask = torch.ones_like(Mask).to(device=self.device, dtype=dataType)
                self.NoMetalS = torch.ones_like(Sc).to(device=self.device, dtype=dataType)
                self._train_mode()
                self.Xc = Xc.to(device=self.device,dtype=dataType)
                self.Sa = Sa.to(device=self.device, dtype=dataType)
                self.SLI = SLI.to(device=self.device, dtype=dataType)
                self.Mask = Mask.to(device=self.device, dtype=dataType)
                self.Sc = Sc.to(device=self.device, dtype=dataType)
                self.XLI = XLI.to(device=self.device, dtype=dataType)
                ############ model forward ##################
                #------------Generator-Xa->Xac---------------
                # Xa->FP->Sa Sa->SENet->Sc+Sm Sc->FBP->X_ic X_ic->INet->Xim
                self.Xa = self.FBPtool(self.Sa).div(self.dataset.imPixScale)
                self.Sac,self.Sm,self.X_sc,self.X_ac,self.X_im = self.DualModle1(self.Sa,self.Mask)
                # ------------Generator-Xc->Xca---------------
                #Sc+Sm=Sca Sca->FBP->Ica Ica+X_im=Xca
                self.Xca,self.Sca = self.DualModle2(self.Sc,self.Sm.detach(), self.X_im.detach())

                # ------------Generator-Xca->Xcac---------------
                self.Scac,self.Sm_cac,self.X_sc_cac,self.Xcac,self.X_im_cac = self.DualModle1(self.Sca,self.Mask)

                # ------------Generator-Xac->Xaca---------------
                Sac1 = self.FPtool(self.X_ac).mul(self.dataset.imPixScale)
                self.Xaca,_ = self.DualModle2(Sac1,self.Sm_cac.detach(),self.X_im_cac.detach())

                ############ model forward end ##################

                ############ model gan loss ##################
                # -----------GA & GB----------------
                self.set_requires_grad([self.netD_Xa,self.netD_Xc],False)

                self.optimizer_G.zero_grad()
                self.Generator_backward()
                self.optimizer_G.step()

                #D_Xa D_Xc
                self.set_requires_grad([self.netD_Xa, self.netD_Xc], True)
                self.optimizer_D.zero_grad()
                self.D_Xa_backward()
                self.D_Xc_backward()
                self.optimizer_D.step()

                ################show the train loss ###################
                temp_GAN_loss.append(self.loss_G.item())
                temp_Dis_loss.append(self.loss_D_Xc.item()+self.loss_D_Xa.item())
                loop.set_description(f'Epoch [{self.opt.epochs}/{epoch}]')
                loop.set_postfix(GANLoss=self.loss_G.item(), G_Xa_Loss=self.loss_G_Xa.item(),
                                 G_Xc_Loss=self.loss_G_Xc.item(), loss_D_Xc=self.loss_D_Xc.item(),
                                 loss_D_Xa=self.loss_D_Xa.item())

            #updata the lr scheduler
            self.gen_writer.add_scalar('loss/gen_whole_loss', np.mean(temp_GAN_loss), epoch)
            self.gen_writer.add_scalar('loss/dis_whole_loss', np.mean(temp_Dis_loss), epoch)
            self.updata_lr_scheduler()
            # val mode
            if self.val_loder:
                self._eval_mode()
                with torch.no_grad():
                    for _,_,_,Mask,Scc,Sma,_ in self.val_loder:
                        torch.cuda.empty_cache()
                        Mask = Mask.to(device=self.device, dtype=dataType)
                        Sma = Sma.to(device=self.device, dtype=dataType)
                        Scc = Scc.to(device=self.device, dtype=dataType)
                        _, aS, X_sc1, X_ac1, aI = self.DecompoNet(Sma, Mask)
                        Xaca, _ = self.CombineNet(Scc, aS, aI)
                        X_ac1 = np.array(X_ac1.data.cpu()[0])[0]
                        X_sc1 = np.array(X_sc1.data.cpu()[0])[0]
                        Xaca = np.array(Xaca.data.cpu()[0])[0]
                        Result_img = {}
                        Result_img['DuDoASNet1'] = X_sc1
                        Result_img['DuDoASNet2'] = X_ac1
                        Result_img['sysmetalimg'] = Xaca
                        savemat('{:s}/{:d}.mat'.format(self.opt.lossmat_path,epoch), Result_img)


            if (epoch)%self.opt.save_epoch_freq==0:
                self._saveModel(epoch)
                loss_logger = {'GAN_loss': self.GAN_loss_logger,'Dis_loss':self.Dis_loss_logger}
                savemat('{:s}/trainloss.mat'.format(self.opt.lossmat_path), mdict=loss_logger)





